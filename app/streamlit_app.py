"""
Streamlit web interface for the RAG chatbot - Docker optimized version.
"""

import streamlit as st
import os
import sys
import time
from pathlib import Path
from typing import List, Dict

# Add the parent directory to Python path
parent_dir = Path(__file__).parent.parent
sys.path.append(str(parent_dir))

# Simple imports
from embeddings.embedding_generator import EmbeddingGenerator
from vectorstore.vector_db import VectorDatabase

# Simple local LLM class for Docker
class LocalLLMFallback:
    """Simple local LLM fallback for Docker deployment."""

    def generate_answer(self, query: str, chunks: List[Dict]) -> str:
        """Generate a simple answer based on retrieved chunks."""
        if not chunks:
            return f"I don't have specific information about '{query}' in the available documents. The documents focus on RAG systems, document processing, and file formats."

        # Get the most relevant chunk
        top_chunk = chunks[0]
        text = top_chunk.get('text', '')
        source = top_chunk.get('document_id', 'documentation')

        # Simple answer generation based on query type
        query_lower = query.lower()

        if 'rag' in query_lower:
            return f"Based on the documentation: RAG (Retrieval-Augmented Generation) is a technique that combines document retrieval with text generation to provide accurate, grounded responses.\n\nFrom {source}: {text[:200]}..."

        elif 'format' in query_lower or 'file' in query_lower:
            return f"According to the documentation: The system supports multiple file formats including PDF, DOCX, TXT, and Markdown files.\n\nFrom {source}: {text[:200]}..."

        elif 'add' in query_lower and 'document' in query_lower:
            return f"To add new documents: Place your files in the data folder and restart the system. The ingestion process will automatically detect and process new files.\n\nFrom {source}: {text[:200]}..."

        elif 'accurate' in query_lower or 'accuracy' in query_lower:
            return f"The RAG system's accuracy depends on the quality of your source documents. Since RAG grounds responses in real content, it's typically much more accurate than standalone language models.\n\nFrom {source}: {text[:200]}..."

        else:
            return f"Based on the available information in {source}:\n\n{text[:300]}..."

# Page configuration
st.set_page_config(
    page_title="ğŸ¤– RAG Document Assistant",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize session state
if 'messages' not in st.session_state:
    st.session_state.messages = []
if 'rag_system' not in st.session_state:
    st.session_state.rag_system = None

class StreamlitRAGSystem:
    """RAG system optimized for Streamlit interface."""

    def __init__(self):
        self.embedder = None
        self.vector_db = None
        self.local_llm = None
        self.is_initialized = False

    def initialize(self):
        """Initialize the RAG system."""
        try:
            with st.spinner("ğŸš€ Loading RAG system..."):
                # Load embedding model
                self.embedder = EmbeddingGenerator(model_type="sentence-bert")

                # Load vector database
                self.vector_db = VectorDatabase.load("vectorstore", "document_vectors")

                # Initialize local LLM
                self.local_llm = LocalLLMFallback()

                self.is_initialized = True
                st.success("âœ… RAG system loaded successfully!")
                return True

        except Exception as e:
            st.error(f"âŒ Failed to load RAG system: {str(e)}")
            st.info("ğŸ’¡ Make sure the vector database exists. In a production setup, this would be automatically created.")
            return False

    def query(self, question: str, top_k: int = 3) -> Dict:
        """Query the RAG system."""
        if not self.is_initialized:
            return {"error": "System not initialized", "success": False}

        try:
            start_time = time.time()

            # Generate query embedding
            query_embedding = self.embedder.generate_embedding(question)

            # Search vector database
            chunks = self.vector_db.search(query_embedding, top_k=top_k)

            # Generate answer
            answer = self.local_llm.generate_answer(question, chunks)

            processing_time = time.time() - start_time

            # Calculate confidence
            confidence = chunks[0].get('similarity_score', 0) if chunks else 0

            return {
                "answer": answer,
                "chunks": chunks,
                "confidence": confidence,
                "processing_time": processing_time,
                "success": True
            }

        except Exception as e:
            return {"error": str(e), "success": False}

def display_chat_interface():
    """Display the main chat interface."""
    st.title("ğŸ¤– RAG Document Assistant")
    st.markdown("Ask me anything about your documents!")

    # Initialize RAG system
    if st.session_state.rag_system is None:
        st.session_state.rag_system = StreamlitRAGSystem()

    if not st.session_state.rag_system.is_initialized:
        if not st.session_state.rag_system.initialize():
            st.stop()

    # Sample questions as buttons
    st.markdown("### ğŸ’¡ Try these sample questions:")
    col1, col2, col3 = st.columns(3)

    sample_questions = [
        "What is RAG?",
        "How do I add new documents?",
        "What file formats are supported?",
        "How accurate is the system?",
        "What are the benefits?",
        "What is quantum computing?"
    ]

    for i, question in enumerate(sample_questions):
        col = [col1, col2, col3][i % 3]
        with col:
            if st.button(question, key=f"sample_{i}"):
                # Add the question to chat
                st.session_state.messages.append({"role": "user", "content": question})
                st.session_state.dummy = st.session_state.get('dummy', 0) + 1

    st.markdown("---")

    # Chat interface
    chat_container = st.container()

    # Display chat history
    with chat_container:
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

                if message["role"] == "assistant" and "metadata" in message:
                    # Show metadata in an expander
                    with st.expander("ğŸ“Š Response Details", expanded=False):
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("Confidence", f"{message['metadata']['confidence']:.1%}")
                        with col2:
                            st.metric("Sources", len(message['metadata']['chunks']))
                        with col3:
                            st.metric("Time", f"{message['metadata']['processing_time']:.2f}s")

                        if message['metadata']['chunks']:
                            st.subheader("ğŸ“š Sources Used:")
                            for i, chunk in enumerate(message['metadata']['chunks'][:3], 1):
                                st.write(
                                    f"**{i}. {chunk.get('document_id', 'Unknown')}** (Relevance: {chunk.get('similarity_score', 0):.1%})")
                                # Show first 150 characters of source text
                                text_preview = chunk.get('text', '')[:150] + "..." if len(
                                    chunk.get('text', '')) > 150 else chunk.get('text', '')
                                st.write(f"ğŸ“„ _{text_preview}_")
                                st.write("---")
    # Chat input
    if prompt := st.chat_input("Ask a question about your documents..."):
        # Add user message
        st.session_state.messages.append({"role": "user", "content": prompt})

        with st.chat_message("user"):
            st.markdown(prompt)

        # Generate response
        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤” Thinking..."):
                response = st.session_state.rag_system.query(prompt)

            if response.get("success", False):
                st.markdown(response["answer"])

                # Add assistant message with metadata
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": response["answer"],
                    "metadata": {
                        "confidence": response["confidence"],
                        "chunks": response["chunks"],
                        "processing_time": response["processing_time"]
                    }
                })
            else:
                error_msg = f"âŒ Error: {response.get('error', 'Unknown error')}"
                st.error(error_msg)
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": error_msg
                })

def display_sidebar():
    """Display the sidebar with system info and controls."""
    st.sidebar.title("ğŸ› ï¸ System Dashboard")

    # System status
    st.sidebar.subheader("ğŸ“Š System Status")

    if st.session_state.rag_system and st.session_state.rag_system.is_initialized:
        st.sidebar.success("âœ… RAG System Online")

        # Get system stats
        try:
            stats = st.session_state.rag_system.vector_db.get_stats()
            st.sidebar.metric("Documents Indexed", stats.get('total_chunks', 0))
            st.sidebar.metric("Vector Dimension", stats.get('embedding_dimension', 0))
            st.sidebar.metric("Index Type", stats.get('index_type', 'Unknown'))
        except:
            st.sidebar.info("Stats unavailable")
    else:
        st.sidebar.warning("âš ï¸ System Loading...")

    # Clear chat button
    st.sidebar.subheader("ğŸ—‘ï¸ Chat Controls")
    if st.sidebar.button("Clear Chat History"):
        st.session_state.messages = []
        st.session_state.dummy = st.session_state.get('dummy', 0) + 1

    # System information
    st.sidebar.subheader("â„¹ï¸ About This RAG System")
    st.sidebar.info("""
    ğŸ” **Semantic Search**: Uses Sentence-BERT embeddings to understand meaning
    
    ğŸ—„ï¸ **Vector Database**: FAISS for lightning-fast similarity search
    
    ğŸ¤– **Local Generation**: Rule-based answer generation (no API costs!)
    
    ğŸ“š **Source Attribution**: Shows which documents were used for each answer
    
    ğŸ³ **Containerized**: Running in Docker for portability
    
    **Built with**: Python, Streamlit, Sentence-Transformers, FAISS
    """)

    # Performance info
    st.sidebar.subheader("âš¡ Performance")
    if st.session_state.rag_system and st.session_state.rag_system.is_initialized:
        st.sidebar.success("ğŸš€ System ready for queries")
        st.sidebar.info("Typical response time: 0.1-2 seconds")

    # Docker info
    st.sidebar.subheader("ğŸ³ Docker Deployment")
    st.sidebar.write("""
    This RAG system is running in a Docker container, making it:
    
    â€¢ **Portable**: Runs anywhere Docker runs
    â€¢ **Consistent**: Same environment everywhere
    â€¢ **Scalable**: Ready for cloud deployment
    â€¢ **Professional**: Industry-standard containerization
    
    Perfect for production deployment!
    """)

def main():
    """Main app function."""
    # Display sidebar
    display_sidebar()

    # Main content
    display_chat_interface()

    # Footer
    st.markdown("---")
    st.markdown(
        "<div style='text-align: center; color: gray; font-size: 0.8em;'>"
        "ğŸ¤– RAG Document Assistant | Running in Docker | Built with Streamlit, Sentence-BERT & FAISS | "
        "Production-ready containerized AI system"
        "</div>",
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()